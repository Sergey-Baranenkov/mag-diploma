{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! pip uninstall numba -y && pip install numba"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Получаем абсолютный путь к корневой директории проекта (директория выше текущей)\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Добавляем корневую директорию в sys.path\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from model_loaders import load_ss_model\n",
    "import weightwatcher as ww\n",
    "from matplotlib import pyplot as plt\n",
    "from pipeline import separate_audio\n",
    "import torch\n",
    "from utils import parse_yaml\n",
    "from models.clap_encoder import CLAP_Encoder\n",
    "import IPython.display as ipd\n",
    "from models.audiosep_lora_and_tuned_embeddings import AudioSepLoraAndTunedEmbeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SS_CONFIG_PATH = '../config/audiosep_base.yaml'\n",
    "CLAP_CKPT_PATH = '../checkpoint/music_speech_audioset_epoch_15_esc_89.98.pt'\n",
    "AUDIOSEP_CKPT_PATH = '../checkpoint/audiosep_base_4M_steps.ckpt'\n",
    "classes = [\"bass\", \"drums\", \"vocals\"]\n",
    "device = torch.device('cuda')\n",
    "configs = parse_yaml(SS_CONFIG_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_encoder = CLAP_Encoder(pretrained_path = CLAP_CKPT_PATH).eval().to(device)\n",
    "base_model = load_ss_model(configs=configs, checkpoint_path=AUDIOSEP_CKPT_PATH, query_encoder=query_encoder).eval().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_path = '../checkpoints/train_audiosep_lora_and_tuned_embeddings/audiosep_lora_and_tuned_embeddings_musdb18,args=logs_per_class=True, dropout=0.1,timestamp=1712871001.7698753/epoch=19.ckpt'\n",
    "\n",
    "query_encoder_for_lora = CLAP_Encoder(pretrained_path = CLAP_CKPT_PATH).eval().to(device)\n",
    "base_model_for_lora = load_ss_model(configs=configs, checkpoint_path=AUDIOSEP_CKPT_PATH, query_encoder=query_encoder_for_lora).eval().to(device)\n",
    "\n",
    "lora_model = AudioSepLoraAndTunedEmbeddings.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    strict=False,\n",
    "    pretrained_audiosep_model = base_model_for_lora,\n",
    "    loss_function=None,\n",
    "    waveform_mixer=None,\n",
    "    lr_lambda_func=None\n",
    ") \\\n",
    "    .eval() \\\n",
    "    .to(device)\n",
    "\n",
    "merged_lora_model = lora_model.model.merge_and_unload()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_files = ['../evaluation/data/musdb18/test/Lyndsey Ollard - Catching Up/mixture.wav']\n",
    "\n",
    "for file in test_files:\n",
    "    display(ipd.Audio(file))\n",
    "    filename = file.split(os.sep)[-1]\n",
    "    for cls in classes:\n",
    "        output_file = f'../separation_result/audiosep_lora_and_tuned_embedings_{filename}_{cls}.wav'\n",
    "        separate_audio(merged_lora_model, file, cls, output_file, device, use_chunk=True)\n",
    "        display(ipd.Audio(output_file))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_details, base_summary = describe_weights(base_model.ss_model)\n",
    "plot_hist(base_details)\n",
    "print(base_summary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lora_details, lora_summary = describe_weights(merged_lora_model.ss_model)\n",
    "plot_hist(lora_details)\n",
    "print(lora_summary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "watcher = ww.WeightWatcher()\n",
    "avg_dW, avg_db, distances = watcher.distances(base_model.ss_model, merged_lora_model.ss_model)\n",
    "avg_dW, avg_db"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(merged_lora_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conv_distances = distances[distances['name']=='Conv2d']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_split = conv_distances['longname'].str.split('.', expand=True)\n",
    "\n",
    "# Объединение разделенных частей с исходным DataFrame\n",
    "df_expanded = pd.concat([df_split, conv_distances['delta_W']], axis=1)\n",
    "\n",
    "# Группировка и суммирование значений\n",
    "# Уровень группировки будет увеличиваться на каждом шаге\n",
    "for level in range(df_expanded.shape[1] - 1):  # Исключаем колонку 'value'\n",
    "    grouped_df = df_expanded.groupby(list(range(level + 1))).sum().reset_index()\n",
    "    grouped_df = grouped_df.sort_values(by='delta_W', ascending=False)\n",
    "    print(f\"Группировка по уровню {level + 1}:\\n\", grouped_df, \"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_details[base_details['warning']!= '']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lora_details[lora_details['warning']!= '']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "watcher = ww.WeightWatcher()\n",
    "layer1_iterator = watcher.make_layer_iterator(model=base_model.ss_model)\n",
    "layer2_iterator = watcher.make_layer_iterator(model=merged_lora_model.ss_model)\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['layer_name', 'base_norm', 'lora_norm', 'norm_of_diff', 'diff_of_norms'])\n",
    "for layer1, layer2 in zip(layer1_iterator, layer2_iterator):\n",
    "    if layer1.name != 'Conv2d':\n",
    "        continue\n",
    "    if layer1.longname != layer2.longname:\n",
    "        raise Exception('layer names are not equal!')\n",
    "\n",
    "    has_weights1, W1, has_biases1, b1  = layer1.get_weights_and_biases()\n",
    "    W1 = W1.astype(np.float32)\n",
    "    has_weights2, W2, has_biases2, b2  = layer2.get_weights_and_biases()\n",
    "    W2 = W2.astype(np.float32)\n",
    "\n",
    "    if has_weights1 and has_weights2:\n",
    "        norm1 = np.linalg.norm(W1)\n",
    "        norm2 = np.linalg.norm(W2)\n",
    "        metrics_df = metrics_df.append({'layer_name': layer1.longname, 'base_norm': norm1, 'lora_norm': norm2, 'norm_of_diff': watcher.matrix_distance(W1, W2), 'diff_of_norms': np.linalg.norm(W1) - np.linalg.norm(W2)}, ignore_index=True)\n",
    "\n",
    "metrics_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "www = torch.load('../checkpoints/train_audiosep_lora_and_tuned_embeddings/audiosep_lora_and_tuned_embeddings_musdb18,timestamp=1710451587.1702235/epoch=29.ckpt')\n",
    "print([v for k, v in www['state_dict'].items() if 'tuned_embedding_layer' in k.lower()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_details, test_summary = describe_weights(www)\n",
    "plot_hist(test_details)\n",
    "print(test_summary)\n",
    "test_details"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
