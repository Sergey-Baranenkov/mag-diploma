{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Визуализация работы FAISS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:02:59.828689670Z",
     "start_time": "2024-04-29T12:02:59.791205839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "# Получаем абсолютный путь к корневой директории проекта (директория выше текущей)\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Добавляем корневую директорию в sys.path\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:02:59.828869622Z",
     "start_time": "2024-04-29T12:02:59.791382495Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 15:03:01.250671: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-29 15:03:01.250702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-29 15:03:01.251622: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-29 15:03:01.256175: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 15:03:01.729296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-29 15:03:04,542 - INFO - Loading faiss with AVX2 support.\n",
      "2024-04-29 15:03:04,550 - INFO - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import parse_yaml\n",
    "from models.clap_encoder import CLAP_Encoder\n",
    "import faiss\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:04.583073828Z",
     "start_time": "2024-04-29T12:02:59.791455002Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SS_CONFIG_PATH = '../config/audiosep_base.yaml'\n",
    "CLAP_CKPT_PATH = '../checkpoint/music_speech_audioset_epoch_15_esc_89.98.pt'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:04.614767957Z",
     "start_time": "2024-04-29T12:03:04.584866982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 15:03:04,847 - INFO - Loading HTSAT-base model config.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-04-29 15:03:06,814 - INFO - Loading pretrained HTSAT-base-roberta weights (../checkpoint/music_speech_audioset_epoch_15_esc_89.98.pt).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "configs = parse_yaml(SS_CONFIG_PATH)\n",
    "\n",
    "query_encoder = CLAP_Encoder(pretrained_path = CLAP_CKPT_PATH).eval().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:07.660539845Z",
     "start_time": "2024-04-29T12:03:04.615980930Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Эмбеддинги, которые сохраняем в бд\n",
    "saved_classes = ['vocal', 'drums', 'guitar', 'hippopotamus', 'roar', 'blender']\n",
    "# Запросы, к которым будем искать ближайший класс из бд\n",
    "query_classes = ['kick', 'ukulele', 'singing', 'howl', 'scream']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:07.700956333Z",
     "start_time": "2024-04-29T12:03:07.661287665Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 512]), torch.Size([5, 512]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_to_save = query_encoder.get_query_embed(modality='text', text=saved_classes).cpu()\n",
    "embeddings_to_query = query_encoder.get_query_embed(modality='text', text=query_classes).cpu()\n",
    "embeddings_to_save.shape, embeddings_to_query.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:07.868594194Z",
     "start_time": "2024-04-29T12:03:07.695199554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "index = faiss.IndexHNSWFlat(512, 32)\n",
    "index.add(embeddings_to_save)\n",
    "distances, indices = index.search(embeddings_to_query, k = len(saved_classes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:07.929079061Z",
     "start_time": "2024-04-29T12:03:07.869689333Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.0701145 , 1.3757946 , 1.4211205 , 1.4345753 , 1.5783528 ,\n        1.6412584 ],\n       [1.1907144 , 1.3707284 , 1.3740044 , 1.4348282 , 1.8569329 ,\n        1.8919489 ],\n       [0.5037271 , 0.9429968 , 0.98835295, 1.4366088 , 1.5765313 ,\n        1.9686406 ],\n       [0.9751475 , 0.9951929 , 1.2816312 , 1.4155016 , 1.5595326 ,\n        1.8042557 ],\n       [1.0844126 , 1.0952895 , 1.2217162 , 1.3817437 , 1.5443419 ,\n        1.5597425 ]], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:07.948131140Z",
     "start_time": "2024-04-29T12:03:07.905199848Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 2, 4, 3, 5],\n       [2, 0, 3, 1, 5, 4],\n       [0, 1, 3, 2, 5, 4],\n       [3, 0, 1, 2, 4, 5],\n       [1, 3, 0, 2, 5, 4]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:07.976524232Z",
     "start_time": "2024-04-29T12:03:07.947989692Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class kick, nearest classes: ['drums__1.07', 'vocal__1.38', 'guitar__1.42', 'roar__1.43', 'hippopotamus__1.58', 'blender__1.64'], difference between top 1 and top2: 0.30568015575408936 difference between top 1 and last: 0.5711438655853271\n",
      "Class ukulele, nearest classes: ['guitar__1.19', 'vocal__1.37', 'hippopotamus__1.37', 'drums__1.43', 'blender__1.86', 'roar__1.89'], difference between top 1 and top2: 0.1800140142440796 difference between top 1 and last: 0.7012345790863037\n",
      "Class singing, nearest classes: ['vocal__0.50', 'drums__0.94', 'hippopotamus__0.99', 'guitar__1.44', 'blender__1.58', 'roar__1.97'], difference between top 1 and top2: 0.4392697215080261 difference between top 1 and last: 1.4649134874343872\n",
      "Class howl, nearest classes: ['hippopotamus__0.98', 'vocal__1.00', 'drums__1.28', 'guitar__1.42', 'roar__1.56', 'blender__1.80'], difference between top 1 and top2: 0.02004539966583252 difference between top 1 and last: 0.8291082382202148\n",
      "Class scream, nearest classes: ['drums__1.08', 'hippopotamus__1.10', 'vocal__1.22', 'guitar__1.38', 'blender__1.54', 'roar__1.56'], difference between top 1 and top2: 0.010876893997192383 difference between top 1 and last: 0.4753298759460449\n"
     ]
    }
   ],
   "source": [
    "for i, query_class in enumerate(query_classes):\n",
    "    k_nearest_indices = indices[i]\n",
    "    k_nearest_distances = distances[i]\n",
    "    class_distance_tuples = [\n",
    "        (saved_classes[class_index], distance) for class_index, distance in zip(k_nearest_indices, k_nearest_distances)]\n",
    "\n",
    "    k_nearest_names = [f'{cls}__{dist:.2f}' for (cls, dist) in class_distance_tuples]\n",
    "    difference_between_top2 = class_distance_tuples[1][1] - class_distance_tuples[0][1]\n",
    "    difference_between_last = class_distance_tuples[len(k_nearest_distances) - 1][1] - class_distance_tuples[0][1]\n",
    "\n",
    "    print(f'Class {query_class}, '\n",
    "          f'nearest classes: {k_nearest_names}, '\n",
    "          f'difference between top 1 and top2: {difference_between_top2} '\n",
    "          f'difference between top 1 and last: {difference_between_last}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:08.018068442Z",
     "start_time": "2024-04-29T12:03:07.969506610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.0000)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(embeddings_to_save[0]).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:03:08.029976709Z",
     "start_time": "2024-04-29T12:03:08.011200424Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вектора нормализованы, по неравенству треугольника максимальное расстояние между векторами = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видно, что энкодер правильно нашел ближайший класс для классов kick, ukulele и singing.\n",
    "\n",
    "Однако для классов howl и scream энкодер не нашел класс roar, являющийся очевидным синонимом. Возможно такого класса не было в словаре у энкодера.\n",
    "\n",
    "Также отметим, что для каждого класса из запроса имелся только один правильный класс, сохраненный в базе данных, однако разница между топ 1 и топ 2 классами для неправильно определенных классов и для класса vocal оказалась незначительной, что может говорить о несовершенстве энкодера.\n",
    "\n",
    "Также для самого близкого класса singing ближайший класс vocal лежит на расстоянии 0.5 (L2 norm). Это намного ближе относительно второго места, однако если посмотреть на первые места сложно определить правильный threshold (когда использовать адаптер, а когда базовую модель) для синонимичных классов.\n",
    "\n",
    "Однако исходя из того, что CLAP является SOTA решением для определения близости captions в контексте звука, решено использовать в качестве эмбеддера именно его."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Также интересно проанализировать близость запросов к классам audioset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "audioset_classes_path = 'ontology.json'\n",
    "with open(audioset_classes_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    names = [x for x in map(lambda x: x['name'], data)]\n",
    "    # добавляем lower-cased названия, дальше будет описано зачем\n",
    "    names += [x.lower() for x in map(lambda x: x['name'], data)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:12:07.569897309Z",
     "start_time": "2024-04-29T12:12:07.502201854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "embeddings_to_save = np.asarray([query_encoder.get_query_embed(modality='text', text=[x]).cpu() for x in names]).squeeze(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:12:29.964132121Z",
     "start_time": "2024-04-29T12:12:08.175942661Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "index = faiss.IndexHNSWFlat(512, 32)\n",
    "index.add(np.asarray(embeddings_to_save))\n",
    "distances, indices = index.search(embeddings_to_query, k = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:12:30.008093920Z",
     "start_time": "2024-04-29T12:12:30.007890404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class kick, nearest classes: ['thump, thud__0.79', 'whack, thwack__0.84', 'Thump, thud__0.86'], difference between top 1 and top2: 0.04906284809112549 difference between top 1 and last: 0.06486690044403076\n",
      "Class ukulele, nearest classes: ['ukulele__0.00', 'Ukulele__0.19', 'Mandolin__0.68'], difference between top 1 and top2: 0.1898217350244522 difference between top 1 and last: 0.6820544004440308\n",
      "Class singing, nearest classes: ['singing__0.00', 'Singing__0.18', 'vocal music__0.47'], difference between top 1 and top2: 0.18427009880542755 difference between top 1 and last: 0.47043418884277344\n",
      "Class howl, nearest classes: ['howl__0.00', 'yawn__0.48', 'hoot__0.59'], difference between top 1 and top2: 0.4791603088378906 difference between top 1 and last: 0.5930732488632202\n",
      "Class scream, nearest classes: ['screaming__0.37', 'battle cry__0.44', 'yell__0.49'], difference between top 1 and top2: 0.07813376188278198 difference between top 1 and last: 0.12256881594657898\n"
     ]
    }
   ],
   "source": [
    "for i, query_class in enumerate(query_classes):\n",
    "    k_nearest_indices = indices[i]\n",
    "    k_nearest_distances = distances[i]\n",
    "    class_distance_tuples = [\n",
    "        (names[class_index], distance) for class_index, distance in zip(k_nearest_indices, k_nearest_distances)]\n",
    "\n",
    "    k_nearest_names = [f'{cls}__{dist:.2f}' for (cls, dist) in class_distance_tuples]\n",
    "    difference_between_top2 = class_distance_tuples[1][1] - class_distance_tuples[0][1]\n",
    "    difference_between_last = class_distance_tuples[len(k_nearest_distances) - 1][1] - class_distance_tuples[0][1]\n",
    "\n",
    "    print(f'Class {query_class}, '\n",
    "          f'nearest classes: {k_nearest_names}, '\n",
    "          f'difference between top 1 and top2: {difference_between_top2} '\n",
    "          f'difference between top 1 and last: {difference_between_last}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T12:12:30.071914139Z",
     "start_time": "2024-04-29T12:12:30.008263382Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Замечаем, что CLAP - case sensitive (ukulele и Ukulele - разные вектора) - видимо при его обучении captions не приводили к нижнему регистру.\n",
    "\n",
    "Также видим, что если пространство классов большое - качество top-k классификации хорошее."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
