{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Визуализация работы FAISS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:36.359267667Z",
     "start_time": "2024-05-06T09:28:36.250093729Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "# Получаем абсолютный путь к корневой директории проекта (директория выше текущей)\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Добавляем корневую директорию в sys.path\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:36.366839599Z",
     "start_time": "2024-05-06T09:28:36.348367043Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 12:28:37,011 - INFO - Loading faiss with AVX2 support.\n",
      "2024-05-06 12:28:37,026 - INFO - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import parse_yaml\n",
    "from models.clap_encoder import CLAP_Encoder\n",
    "import faiss\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:37.032568621Z",
     "start_time": "2024-05-06T09:28:36.348559176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "SS_CONFIG_PATH = '../config/audiosep_base.yaml'\n",
    "CLAP_CKPT_PATH = '../checkpoint/music_speech_audioset_epoch_15_esc_89.98.pt'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:37.075160707Z",
     "start_time": "2024-05-06T09:28:37.034504670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 12:28:37,291 - INFO - Loading HTSAT-base model config.\n",
      "2024-05-06 12:28:42,325 - INFO - Loading pretrained HTSAT-base-roberta weights (../checkpoint/music_speech_audioset_epoch_15_esc_89.98.pt).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "configs = parse_yaml(SS_CONFIG_PATH)\n",
    "\n",
    "query_encoder = CLAP_Encoder(pretrained_path = CLAP_CKPT_PATH).eval().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:44.673538263Z",
     "start_time": "2024-05-06T09:28:37.075431599Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Эмбеддинги, которые сохраняем в бд\n",
    "saved_classes = ['vocal', 'drums', 'guitar', 'hippopotamus', 'roar', 'blender']\n",
    "# Запросы, к которым будем искать ближайший класс из бд\n",
    "query_classes = ['kick', 'ukulele', 'singing', 'howl', 'scream']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:44.718651093Z",
     "start_time": "2024-05-06T09:28:44.718371074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 512]), torch.Size([5, 512]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_to_save = query_encoder.get_query_embed(modality='text', text=saved_classes).cpu()\n",
    "embeddings_to_query = query_encoder.get_query_embed(modality='text', text=query_classes).cpu()\n",
    "embeddings_to_save.shape, embeddings_to_query.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:44.920995098Z",
     "start_time": "2024-05-06T09:28:44.718892900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "index = faiss.IndexHNSWFlat(512, 32)\n",
    "index.add(embeddings_to_save)\n",
    "distances, indices = index.search(embeddings_to_query, k = len(saved_classes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:44.962882492Z",
     "start_time": "2024-05-06T09:28:44.920777146Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.0701145 , 1.3757946 , 1.4211205 , 1.4345753 , 1.5783528 ,\n        1.6412584 ],\n       [1.1907144 , 1.3707284 , 1.3740044 , 1.4348282 , 1.8569329 ,\n        1.8919489 ],\n       [0.5037271 , 0.9429968 , 0.98835295, 1.4366088 , 1.5765313 ,\n        1.9686406 ],\n       [0.9751475 , 0.9951929 , 1.2816312 , 1.4155016 , 1.5595326 ,\n        1.8042557 ],\n       [1.0844126 , 1.0952895 , 1.2217162 , 1.3817437 , 1.5443419 ,\n        1.5597425 ]], dtype=float32)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:44.988024094Z",
     "start_time": "2024-05-06T09:28:44.961684859Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 2, 4, 3, 5],\n       [2, 0, 3, 1, 5, 4],\n       [0, 1, 3, 2, 5, 4],\n       [3, 0, 1, 2, 4, 5],\n       [1, 3, 0, 2, 5, 4]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:45.030630779Z",
     "start_time": "2024-05-06T09:28:44.988180250Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class kick, nearest classes: ['drums__1.07', 'vocal__1.38', 'guitar__1.42', 'roar__1.43', 'hippopotamus__1.58', 'blender__1.64'], difference between top 1 and top2: 0.30568015575408936 difference between top 1 and last: 0.5711438655853271\n",
      "Class ukulele, nearest classes: ['guitar__1.19', 'vocal__1.37', 'hippopotamus__1.37', 'drums__1.43', 'blender__1.86', 'roar__1.89'], difference between top 1 and top2: 0.1800140142440796 difference between top 1 and last: 0.7012345790863037\n",
      "Class singing, nearest classes: ['vocal__0.50', 'drums__0.94', 'hippopotamus__0.99', 'guitar__1.44', 'blender__1.58', 'roar__1.97'], difference between top 1 and top2: 0.4392697215080261 difference between top 1 and last: 1.4649134874343872\n",
      "Class howl, nearest classes: ['hippopotamus__0.98', 'vocal__1.00', 'drums__1.28', 'guitar__1.42', 'roar__1.56', 'blender__1.80'], difference between top 1 and top2: 0.02004539966583252 difference between top 1 and last: 0.8291082382202148\n",
      "Class scream, nearest classes: ['drums__1.08', 'hippopotamus__1.10', 'vocal__1.22', 'guitar__1.38', 'blender__1.54', 'roar__1.56'], difference between top 1 and top2: 0.010876893997192383 difference between top 1 and last: 0.4753298759460449\n"
     ]
    }
   ],
   "source": [
    "for i, query_class in enumerate(query_classes):\n",
    "    k_nearest_indices = indices[i]\n",
    "    k_nearest_distances = distances[i]\n",
    "    class_distance_tuples = [\n",
    "        (saved_classes[class_index], distance) for class_index, distance in zip(k_nearest_indices, k_nearest_distances)]\n",
    "\n",
    "    k_nearest_names = [f'{cls}__{dist:.2f}' for (cls, dist) in class_distance_tuples]\n",
    "    difference_between_top2 = class_distance_tuples[1][1] - class_distance_tuples[0][1]\n",
    "    difference_between_last = class_distance_tuples[len(k_nearest_distances) - 1][1] - class_distance_tuples[0][1]\n",
    "\n",
    "    print(f'Class {query_class}, '\n",
    "          f'nearest classes: {k_nearest_names}, '\n",
    "          f'difference between top 1 and top2: {difference_between_top2} '\n",
    "          f'difference between top 1 and last: {difference_between_last}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:45.054645370Z",
     "start_time": "2024-05-06T09:28:45.028441541Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.0000)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(embeddings_to_save[0]).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:45.089857657Z",
     "start_time": "2024-05-06T09:28:45.050988838Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вектора нормализованы, по неравенству треугольника максимальное расстояние между векторами = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видно, что энкодер правильно нашел ближайший класс для классов kick, ukulele и singing.\n",
    "\n",
    "Однако для классов howl и scream энкодер не нашел класс roar, являющийся очевидным синонимом. Возможно такого класса не было в словаре у энкодера.\n",
    "\n",
    "Также отметим, что для каждого класса из запроса имелся только один правильный класс, сохраненный в базе данных, однако разница между топ 1 и топ 2 классами для неправильно определенных классов и для класса vocal оказалась незначительной, что может говорить о несовершенстве энкодера.\n",
    "\n",
    "Также для самого близкого класса singing ближайший класс vocal лежит на расстоянии 0.5 (L2 norm). Это намного ближе относительно второго места, однако если посмотреть на первые места сложно определить правильный threshold (когда использовать адаптер, а когда базовую модель) для синонимичных классов.\n",
    "\n",
    "Однако исходя из того, что CLAP является SOTA решением для определения близости captions в контексте звука, решено использовать в качестве эмбеддера именно его."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Также интересно проанализировать близость запросов к классам audioset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "audioset_classes_path = 'ontology.json'\n",
    "with open(audioset_classes_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    names = [x for x in map(lambda x: x['name'], data)]\n",
    "    # добавляем lower-cased названия, дальше будет описано зачем\n",
    "    names += [x.lower() for x in map(lambda x: x['name'], data)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:45.126050929Z",
     "start_time": "2024-05-06T09:28:45.083812341Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "embeddings_to_save = np.asarray([query_encoder.get_query_embed(modality='text', text=[x]).cpu() for x in names]).squeeze(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:29:07.114848543Z",
     "start_time": "2024-05-06T09:28:45.115743116Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "index = faiss.IndexHNSWFlat(512, 32)\n",
    "index.add(np.asarray(embeddings_to_save))\n",
    "distances, indices = index.search(embeddings_to_query, k = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:29:07.157936767Z",
     "start_time": "2024-05-06T09:29:07.115533678Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class kick, nearest classes: ['thump, thud__0.79', 'whack, thwack__0.84', 'Thump, thud__0.86'], difference between top 1 and top2: 0.04906284809112549 difference between top 1 and last: 0.06486690044403076\n",
      "Class ukulele, nearest classes: ['ukulele__0.00', 'Ukulele__0.19', 'Mandolin__0.68'], difference between top 1 and top2: 0.1898217350244522 difference between top 1 and last: 0.6820544004440308\n",
      "Class singing, nearest classes: ['singing__0.00', 'Singing__0.18', 'vocal music__0.47'], difference between top 1 and top2: 0.18427009880542755 difference between top 1 and last: 0.47043418884277344\n",
      "Class howl, nearest classes: ['howl__0.00', 'yawn__0.48', 'hoot__0.59'], difference between top 1 and top2: 0.4791603088378906 difference between top 1 and last: 0.5930732488632202\n",
      "Class scream, nearest classes: ['screaming__0.37', 'battle cry__0.44', 'yell__0.49'], difference between top 1 and top2: 0.07813376188278198 difference between top 1 and last: 0.12256881594657898\n"
     ]
    }
   ],
   "source": [
    "def overview_query_result(query_classes, indices, distances):\n",
    "    for i, query_class in enumerate(query_classes):\n",
    "        k_nearest_indices = indices[i]\n",
    "        k_nearest_distances = distances[i]\n",
    "        class_distance_tuples = [\n",
    "            (names[class_index], distance) for class_index, distance in zip(k_nearest_indices, k_nearest_distances)]\n",
    "\n",
    "        k_nearest_names = [f'{cls}__{dist:.2f}' for (cls, dist) in class_distance_tuples]\n",
    "        difference_between_top2 = class_distance_tuples[1][1] - class_distance_tuples[0][1]\n",
    "        difference_between_last = class_distance_tuples[len(k_nearest_distances) - 1][1] - class_distance_tuples[0][1]\n",
    "\n",
    "        print(f'Class {query_class}, '\n",
    "              f'nearest classes: {k_nearest_names}, '\n",
    "              f'difference between top 1 and top2: {difference_between_top2} '\n",
    "              f'difference between top 1 and last: {difference_between_last}')\n",
    "\n",
    "overview_query_result(query_classes, indices, distances)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:35:53.429144300Z",
     "start_time": "2024-05-06T09:35:53.391489035Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Замечаем, что CLAP - case sensitive (ukulele и Ukulele - разные вектора) - видимо при его обучении captions не приводили к нижнему регистру.\n",
    "\n",
    "Также видим, что если пространство классов большое - качество top-k классификации хорошее."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Сравним ближайшие классы из musdb для базовой модели AudioSep и лучшей embeddings модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 12:31:17,874 - INFO - Loading HTSAT-base model config.\n",
      "2024-05-06 12:31:19,174 - INFO - Loading pretrained HTSAT-base-roberta weights (../checkpoint/music_speech_audioset_epoch_15_esc_89.98.pt).\n"
     ]
    }
   ],
   "source": [
    "from models.audiosep_tunned_embeddings import AudioSepTunedEmbeddings\n",
    "from model_loaders import load_ss_model\n",
    "\n",
    "SS_CONFIG_PATH = '../config/audiosep_base.yaml'\n",
    "CLAP_CKPT_PATH = '../checkpoint/music_speech_audioset_epoch_15_esc_89.98.pt'\n",
    "AUDIOSEP_CKPT_PATH = '../checkpoint/audiosep_base_4M_steps.ckpt'\n",
    "device = torch.device('cuda')\n",
    "configs = parse_yaml(SS_CONFIG_PATH)\n",
    "\n",
    "checkpoint_path = '../checkpoints/final/musdb18/embeddings/final.ckpt'\n",
    "\n",
    "query_encoder_for_lora = CLAP_Encoder(pretrained_path = CLAP_CKPT_PATH).eval().to(device)\n",
    "base_model_for_lora = load_ss_model(configs=configs, checkpoint_path=AUDIOSEP_CKPT_PATH, query_encoder=query_encoder_for_lora).eval().to(device)\n",
    "\n",
    "model = AudioSepTunedEmbeddings.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    strict=False,\n",
    "    ss_model=base_model_for_lora.ss_model,\n",
    "    query_encoder=base_model_for_lora.query_encoder,\n",
    "    waveform_mixer=None,\n",
    "    loss_function=None,\n",
    "    optimizer_type=None,\n",
    "    learning_rate=None,\n",
    "    lr_lambda_func=None,\n",
    ") \\\n",
    "    .eval() \\\n",
    "    .to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:31:22.037201180Z",
     "start_time": "2024-05-06T09:31:17.648814097Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class bass, nearest classes: ['chord__0.46', 'recording__0.47', 'zing__0.47'], difference between top 1 and top2: 0.013088047504425049 difference between top 1 and last: 0.017460674047470093\n",
      "Class drums, nearest classes: ['drum__0.23', 'music__0.35', 'chord__0.35'], difference between top 1 and top2: 0.11751864850521088 difference between top 1 and last: 0.12226851284503937\n",
      "Class vocals, nearest classes: ['bellow__0.41', 'music role__0.46', 'music__0.48'], difference between top 1 and top2: 0.05037081241607666 difference between top 1 and last: 0.06213861703872681\n",
      "Class other musical instruments, nearest classes: ['music mood__0.35', 'exciting music__0.46', 'dance music__0.50'], difference between top 1 and top2: 0.11006090044975281 difference between top 1 and last: 0.1472373604774475\n"
     ]
    }
   ],
   "source": [
    "classes = [\"bass\", \"drums\", \"vocals\", 'other musical instruments']\n",
    "\n",
    "base_embeddings = np.asarray([query_encoder.get_query_embed(modality='text', text=[x]).cpu() for x in classes]).squeeze(1)\n",
    "\n",
    "tuned_embeddings = np.asarray([model.query_encoder.get_query_embed(modality='text', text=[x]).detach().cpu() for x in classes]).squeeze(1)\n",
    "\n",
    "base_distances, base_indices = index.search(base_embeddings, k = 3)\n",
    "tuned_distances, tuned_indices = index.search(tuned_embeddings, k = 3)\n",
    "\n",
    "overview_query_result(classes, base_indices, base_distances)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:44:10.815883745Z",
     "start_time": "2024-05-06T09:44:10.626395460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class bass, nearest classes: ['Bass guitar__14.33', 'Bathtub (filling or washing)__14.42', 'pizzicato__14.44'], difference between top 1 and top2: 0.09071826934814453 difference between top 1 and last: 0.11366748809814453\n",
      "Class drums, nearest classes: ['cymbal__10.32', 'crash cymbal__10.38', 'Crash cymbal__10.38'], difference between top 1 and top2: 0.05514717102050781 difference between top 1 and last: 0.060451507568359375\n",
      "Class vocals, nearest classes: ['choir__7.14', 'chant__7.31', 'sigh__7.31'], difference between top 1 and top2: 0.17082548141479492 difference between top 1 and last: 0.17502498626708984\n",
      "Class other musical instruments, nearest classes: ['choir__11.66', 'Civil defense siren__11.91', 'opera__11.98'], difference between top 1 and top2: 0.24882984161376953 difference between top 1 and last: 0.3181324005126953\n"
     ]
    }
   ],
   "source": [
    "overview_query_result(classes, tuned_indices, tuned_distances)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:44:11.141016983Z",
     "start_time": "2024-05-06T09:44:11.107426301Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видим, что классы становятся ближе к похожим на них описаниям. С другой стороны, вектор для класс drums ближе к классу cymbal (тарелки барабанов), чем к существующему в audioset классу drums. Возможно, это эффект переобучения - очень низкие частоты модель определяет в класс bass, в то время как более высокие - в класс drums. Это частично подтверждается эмперически - цокоющие высокие звуки вокала / других музыкальных инструментов модель добавляет в аудиозапись класса drums."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Также заетим, что затюненые вектора перестают быть нормализованными."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.99999994, 1.0, 1.0, 1.0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[3.8561718, 3.263241, 2.6703308, 3.4665463]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display([np.linalg.norm(x) for x in base_embeddings])\n",
    "display([np.linalg.norm(x) for x in tuned_embeddings])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:47:19.085636868Z",
     "start_time": "2024-05-06T09:47:19.044485645Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Посмотрим, не превращает ли энкодер неизвестные слова в одинаковые вектора"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0.70498794"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = query_encoder.get_query_embed(modality='text', text=['wioewjmfoermfp weofkmqew']).cpu()\n",
    "b = query_encoder.get_query_embed(modality='text', text=['dvfdfdbsfbs dasd']).cpu()\n",
    "np.linalg.norm(a-b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T19:51:08.842807835Z",
     "start_time": "2024-04-29T19:51:08.764288762Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Не превращает, поэтому даже неизвестные caption можно использовать при дообучении"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
